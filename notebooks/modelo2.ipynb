{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomResNet50:\n",
    "    def __init__(self, num_classes):\n",
    "        self.model = models.resnet50(pretrained=True)\n",
    "        num_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        data_loader_treino,\n",
    "        data_loader_validacao,\n",
    "        num_imagens_treino,\n",
    "        num_imagens_validacao,\n",
    "        device,\n",
    "        num_classes=8,\n",
    "        patience=5,\n",
    "        nameModel='nome do arquivo.pt',\n",
    "        otimizador=None,\n",
    "        scheduler=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inicializa o Trainer com opções para usar ou não otimizador, scheduler e patience.\n",
    "\n",
    "        Parâmetros:\n",
    "        - model: O modelo a ser treinado.\n",
    "        - data_loader_treino: DataLoader para o conjunto de treino.\n",
    "        - data_loader_validacao: DataLoader para o conjunto de validação.\n",
    "        - num_imagens_treino: Número total de imagens de treino.\n",
    "        - num_imagens_validacao: Número total de imagens de validação.\n",
    "        - device: Dispositivo onde o modelo será treinado (CPU ou GPU).\n",
    "        - num_classes: Número de classes de saída.\n",
    "        - patience: Número de épocas para o early stopping (opcional, pode ser None).\n",
    "        - nameModel: Nome do arquivo para salvar o melhor modelo.\n",
    "        - otimizador: O otimizador a ser usado (opcional, pode ser None).\n",
    "        - scheduler: O scheduler de learning rate a ser usado (opcional, pode ser None).\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.data_loader_treino = data_loader_treino\n",
    "        self.data_loader_validacao = data_loader_validacao\n",
    "        self.num_imagens_treino = num_imagens_treino\n",
    "        self.num_imagens_validacao = num_imagens_validacao\n",
    "        self.device = device\n",
    "        self.funcao_erro = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Inicializa o otimizador apenas se for fornecido, senão cria um padrão\n",
    "        if otimizador is None:\n",
    "            self.otimizador = optim.Adam(self.model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "        else:\n",
    "            self.otimizador = otimizador\n",
    "        \n",
    "        # Inicializa o scheduler apenas se for fornecido\n",
    "        self.scheduler = scheduler\n",
    "        \n",
    "        # Inicializa o patience\n",
    "        self.patience = patience\n",
    "\n",
    "        # Atualiza o caminho para salvar o modelo na pasta 'models/'\n",
    "        self.nameModel = nameModel\n",
    "        self.model_save_path = os.path.join('models', self.nameModel)\n",
    "        os.makedirs(os.path.dirname(self.model_save_path), exist_ok=True)\n",
    "\n",
    "    def treinar_e_validar(self, epocas):\n",
    "        historico = []\n",
    "        melhor_acuracia = 0.0\n",
    "        early_stop_counter = 0\n",
    "\n",
    "        for epoca in range(epocas):\n",
    "            inicio_epoca = time.time()\n",
    "            print(f\"\\n\\nÉpoca: {epoca + 1}/{epocas}\")\n",
    "            erro_treino, acuracia_treino = self.executar_fase('treino')\n",
    "            erro_validacao, acuracia_validacao, predicoes_validacao, labels_validacao = self.executar_fase('validacao', return_predictions=True)\n",
    "\n",
    "            fim_epoca = time.time()\n",
    "            print(f\"Época {epoca + 1}/{epocas}, Treino: Erro: {erro_treino:.4f}, Acurácia: {acuracia_treino * 100:.2f}%, \"\n",
    "                  f\"Validação: Erro: {erro_validacao:.4f}, Acurácia: {acuracia_validacao * 100:.2f}%, Tempo: {fim_epoca - inicio_epoca:.2f}s\")\n",
    "\n",
    "            historico.append([erro_treino, erro_validacao, acuracia_treino, acuracia_validacao])\n",
    "            \n",
    "            # Atualiza o scheduler se ele estiver definido e o scheduler não for None\n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step(erro_validacao)\n",
    "\n",
    "            # Early stopping\n",
    "            if acuracia_validacao > melhor_acuracia:\n",
    "                melhor_acuracia = acuracia_validacao\n",
    "                print(f\"Validation accuracy improved to {melhor_acuracia:.4f}. Saving the model.\")\n",
    "                try:\n",
    "                    torch.save(self.model.state_dict(), self.model_save_path)\n",
    "                    print(f\"Modelo salvo com sucesso em {self.model_save_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro ao salvar o modelo: {e}\")\n",
    "                early_stop_counter = 0\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "\n",
    "            if self.patience is not None and early_stop_counter >= self.patience:\n",
    "                print(\"Parando o treinamento devido ao early stopping.\")\n",
    "                break\n",
    "\n",
    "        # Calcular métricas finais\n",
    "        self.calcular_metricas(predicoes_validacao, labels_validacao)\n",
    "        return historico\n",
    "\n",
    "    def treinar_e_validar(self, epocas):\n",
    "        historico = []\n",
    "        melhor_acuracia = 0.0\n",
    "        early_stop_counter = 0\n",
    "\n",
    "        for epoca in range(epocas):\n",
    "            inicio_epoca = time.time()\n",
    "            print(f\"\\n\\nÉpoca: {epoca + 1}/{epocas}\")\n",
    "            erro_treino, acuracia_treino = self.executar_fase('treino')\n",
    "            erro_validacao, acuracia_validacao, predicoes_validacao, labels_validacao = self.executar_fase('validacao', return_predictions=True)\n",
    "\n",
    "            fim_epoca = time.time()\n",
    "            print(f\"Época {epoca + 1}/{epocas}, Treino: Erro: {erro_treino:.4f}, Acurácia: {acuracia_treino * 100:.2f}%, \"\n",
    "                  f\"Validação: Erro: {erro_validacao:.4f}, Acurácia: {acuracia_validacao * 100:.2f}%, Tempo: {fim_epoca - inicio_epoca:.2f}s\")\n",
    "\n",
    "            historico.append([erro_treino, erro_validacao, acuracia_treino, acuracia_validacao])\n",
    "            \n",
    "            # Atualiza o scheduler se ele estiver definido\n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step(erro_validacao)\n",
    "\n",
    "            # Early stopping\n",
    "            if acuracia_validacao > melhor_acuracia:\n",
    "                melhor_acuracia = acuracia_validacao\n",
    "                print(f\"Validation accuracy improved to {melhor_acuracia:.4f}. Saving the model.\")\n",
    "                try:\n",
    "                    torch.save(self.model.state_dict(), self.model_save_path)\n",
    "                    print(f\"Modelo salvo com sucesso em {self.model_save_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro ao salvar o modelo: {e}\")\n",
    "                early_stop_counter = 0\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "\n",
    "            if self.patience is not None and early_stop_counter >= self.patience:\n",
    "                print(\"Parando o treinamento devido ao early stopping.\")\n",
    "                break\n",
    "\n",
    "        # Calcular métricas finais\n",
    "        self.calcular_metricas(predicoes_validacao, labels_validacao)\n",
    "        return historico\n",
    "\n",
    "    def executar_fase(self, fase, return_predictions=False):\n",
    "        if fase == 'treino':\n",
    "            self.model.train()\n",
    "            data_loader = self.data_loader_treino\n",
    "            num_imagens = self.num_imagens_treino\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            data_loader = self.data_loader_validacao\n",
    "            num_imagens = self.num_imagens_validacao\n",
    "\n",
    "        erro_total = 0.0\n",
    "        acuracia_total = 0.0\n",
    "        todas_predicoes = []\n",
    "        todas_labels = []\n",
    "\n",
    "        with torch.set_grad_enabled(fase == 'treino'):\n",
    "            print(f\"\\nExecutando a fase de {fase}...\")\n",
    "            for entradas, labels in data_loader:\n",
    "                entradas, labels = entradas.to(self.device), labels.to(self.device)\n",
    "\n",
    "                if fase == 'treino':\n",
    "                    self.otimizador.zero_grad()\n",
    "\n",
    "                saidas = self.model(entradas)\n",
    "                erro = self.funcao_erro(saidas, labels)\n",
    "\n",
    "                if fase == 'treino':\n",
    "                    erro.backward()\n",
    "                    self.otimizador.step()\n",
    "                erro_total += erro.item() * entradas.size(0)\n",
    "                _, predicoes = torch.max(saidas, 1)\n",
    "                acuracia_total += (predicoes == labels).sum().item()\n",
    "\n",
    "                if return_predictions:\n",
    "                    todas_predicoes.extend(predicoes.cpu().numpy())\n",
    "                    todas_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "\n",
    "        erro_medio = erro_total / num_imagens\n",
    "        acuracia_media = acuracia_total / num_imagens\n",
    "\n",
    "        if return_predictions:\n",
    "            return erro_medio, acuracia_media, todas_predicoes, todas_labels\n",
    "        else:\n",
    "            return erro_medio, acuracia_media\n",
    "\n",
    "    def calcular_metricas(self, predicoes, labels):\n",
    "        acuracia = accuracy_score(labels, predicoes)\n",
    "        precisao = precision_score(labels, predicoes, average='weighted', zero_division=0)\n",
    "        recall = recall_score(labels, predicoes, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(labels, predicoes, average='weighted', zero_division=0)\n",
    "\n",
    "        print(\"\\nMétricas de Validação:\")\n",
    "        print(f\"Acurácia: {acuracia:.4f}\")\n",
    "        print(f\"Precisão: {precisao:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "        print(\"\\nRelatório de Classificação:\")\n",
    "        print(classification_report(labels, predicoes, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "class DataLoaderSetup:\n",
    "    def __init__(self, dataset_path, image_size, batch_size, transformacoes=None):\n",
    "        \"\"\"\n",
    "        Inicializa a classe DataLoaderSetup.\n",
    "\n",
    "        Parâmetros:\n",
    "        - dataset_path: Caminho para o dataset.\n",
    "        - image_size: Tamanho da imagem para redimensionamento.\n",
    "        - batch_size: Tamanho do batch.\n",
    "        - transformacoes: Dicionário opcional com as transformações para 'treino' e 'validacao'. Se None, serão usadas transformações padrão.\n",
    "        \"\"\"\n",
    "        self.dataset_path = dataset_path\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Se não forem fornecidas transformações, usa as transformações padrão\n",
    "        self.transformacoes = transformacoes or self.get_default_transforms()\n",
    "\n",
    "    def get_default_transforms(self):\n",
    "        \"\"\"\n",
    "        Define transformações padrão para o conjunto de treino e validação.\n",
    "        \"\"\"\n",
    "        transformacoes_de_imagens = {\n",
    "            'treino': transforms.Compose([\n",
    "                transforms.Resize(self.image_size),\n",
    "                transforms.CenterCrop(self.image_size)\n",
    "            ]),\n",
    "            'validacao': transforms.Compose([\n",
    "                transforms.Resize(self.image_size),\n",
    "                transforms.CenterCrop(self.image_size)\n",
    "            ])\n",
    "        }\n",
    "        return transformacoes_de_imagens\n",
    "\n",
    "    def get_data_loaders(self):\n",
    "        \"\"\"\n",
    "        Cria DataLoaders para os conjuntos de treino e validação, aplicando as transformações fornecidas ou padrão.\n",
    "        \"\"\"\n",
    "        # Usa as transformações fornecidas ou as padrão definidas no init\n",
    "        train_dataset = datasets.ImageFolder(os.path.join(self.dataset_path, 'treino'), transform=self.transformacoes['treino'])\n",
    "        val_dataset = datasets.ImageFolder(os.path.join(self.dataset_path, 'validacao'), transform=self.transformacoes['validacao'])\n",
    "\n",
    "        data_loader_treino = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        data_loader_validacao = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        num_imagens_treino = len(train_dataset)\n",
    "        num_imagens_validacao = len(val_dataset)\n",
    "        num_classes = len(train_dataset.classes)\n",
    "\n",
    "        return data_loader_treino, data_loader_validacao, num_imagens_treino, num_imagens_validacao, num_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo utilizado: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Época: 1/30\n",
      "\n",
      "Executando a fase de treino...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, data_loader_treino, data_loader_validacao, num_imagens_treino, num_imagens_validacao, device, num_classes, patience, nameModel,optimizer)\n\u001b[0;32m     46\u001b[0m epocas \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[1;32m---> 47\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtreinar_e_validar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepocas\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 121\u001b[0m, in \u001b[0;36mTrainer.treinar_e_validar\u001b[1;34m(self, epocas)\u001b[0m\n\u001b[0;32m    119\u001b[0m inicio_epoca \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mÉpoca: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoca\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepocas\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m erro_treino, acuracia_treino \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecutar_fase\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtreino\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m erro_validacao, acuracia_validacao, predicoes_validacao, labels_validacao \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecutar_fase(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidacao\u001b[39m\u001b[38;5;124m'\u001b[39m, return_predictions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    124\u001b[0m fim_epoca \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[1;32mIn[3], line 172\u001b[0m, in \u001b[0;36mTrainer.executar_fase\u001b[1;34m(self, fase, return_predictions)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(fase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtreino\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExecutando a fase de \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfase\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m entradas, labels \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[0;32m    173\u001b[0m         entradas, labels \u001b[38;5;241m=\u001b[39m entradas\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    175\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m fase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtreino\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\folder.py:245\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    244\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[1;32m--> 245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[1;32mc:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\folder.py:284\u001b[0m, in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\folder.py:263\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpil_loader\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image\u001b[38;5;241m.\u001b[39mImage:\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 263\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    264\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:3375\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3367\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pixels \u001b[38;5;241m>\u001b[39m MAX_IMAGE_PIXELS:\n\u001b[0;32m   3368\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   3369\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpixels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m pixels) exceeds limit of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_IMAGE_PIXELS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m pixels, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3370\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould be decompression bomb DOS attack.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3371\u001b[0m             DecompressionBombWarning,\n\u001b[0;32m   3372\u001b[0m         )\n\u001b[1;32m-> 3375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(\n\u001b[0;32m   3376\u001b[0m     fp: StrOrBytesPath \u001b[38;5;241m|\u001b[39m IO[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[0;32m   3377\u001b[0m     mode: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3378\u001b[0m     formats: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   3379\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ImageFile\u001b[38;5;241m.\u001b[39mImageFile:\n\u001b[0;32m   3380\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3381\u001b[0m \u001b[38;5;124;03m    Opens and identifies the given image file.\u001b[39;00m\n\u001b[0;32m   3382\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3406\u001b[0m \u001b[38;5;124;03m    :exception TypeError: If ``formats`` is not ``None``, a list or a tuple.\u001b[39;00m\n\u001b[0;32m   3407\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   3409\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from torchvision import transforms\n",
    "# Adicionar o caminho raiz do projeto ao sys.path\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Caminho do dataset\n",
    "dataset_path = r'F:\\Git\\Teste\\FER\\affectnet\\affectnet2'\n",
    "\n",
    "# Definir dispositivo\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Dispositivo utilizado: {device}\")\n",
    "\n",
    "# Transformações personalizadas\n",
    "transformacoes_personalizadas = {\n",
    "    'treino': transforms.Compose([\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'validacao': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "\n",
    "# Preparar DataLoaders\n",
    "data_loader_setup = data_loader_setup = DataLoaderSetup(dataset_path,image_size=224,batch_size=32,transformacoes=transformacoes_personalizadas)\n",
    "data_loader_treino, data_loader_validacao, num_imagens_treino, num_imagens_validacao, num_classes = data_loader_setup.get_data_loaders()\n",
    "\n",
    "# Carregar o modelo\n",
    "model = CustomResNet50(num_classes).get_model().to(device)\n",
    "\n",
    "# Nome do modelo salvo e paciência para early stopping\n",
    "nameModel = 'affectnet.pt'\n",
    "patience = 5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "# Treinar e validar\n",
    "trainer = Trainer(model, data_loader_treino, data_loader_validacao, num_imagens_treino, num_imagens_validacao, device, num_classes, patience, nameModel,optimizer)\n",
    "epocas = 30\n",
    "trainer.treinar_e_validar(epocas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
